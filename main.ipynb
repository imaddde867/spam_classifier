%%writefile spam_detection.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam Detection Notebook\n",
    "## Complete Analysis with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installations (if needed)\n",
    "# !pip install wordcloud seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import email\n",
    "import hashlib\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import urlretrieve\n",
    "from email import policy\n",
    "from email.utils import parsedate_tz\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = 'data'\n",
    "HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2'\n",
    "SPAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2'\n",
    "\n",
    "# Download and extract data\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(\"Downloading datasets...\")\n",
    "    \n",
    "    ham_path, _ = urlretrieve(HAM_URL, 'easy_ham.tar.bz2')\n",
    "    spam_path, _ = urlretrieve(SPAM_URL, 'spam.tar.bz2')\n",
    "\n",
    "    print(\"Extracting files...\")\n",
    "    with tarfile.open(ham_path, 'r:bz2') as tar:\n",
    "        tar.extractall(DATA_DIR)\n",
    "    with tarfile.open(spam_path, 'r:bz2') as tar:\n",
    "        tar.extractall(DATA_DIR)\n",
    "else:\n",
    "    print(\"Data directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    \"\"\"Load email filenames\"\"\"\n",
    "    ham_dir = os.path.join(DATA_DIR, 'easy_ham')\n",
    "    spam_dir = os.path.join(DATA_DIR, 'spam')\n",
    "    \n",
    "    ham_files = [f for f in os.listdir(ham_dir) if not f.startswith('.')]\n",
    "    spam_files = [f for f in os.listdir(spam_dir) if not f.startswith('.')]\n",
    "    \n",
    "    return ham_files, spam_files\n",
    "\n",
    "ham_files, spam_files = load_files()\n",
    "print(f\"Loaded {len(ham_files)} ham emails\")\n",
    "print(f\"Loaded {len(spam_files)} spam emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'filename': ham_files + spam_files,\n",
    "    'is_spam': [0]*len(ham_files) + [1]*len(spam_files)\n",
    "})\n",
    "\n",
    "# Generate UUIDs\n",
    "df['email_id'] = df['filename'].apply(\n",
    "    lambda x: hashlib.sha256(x.encode()).hexdigest()[:16]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_content(row):\n",
    "    \"\"\"Extract headers and content from raw email\"\"\"\n",
    "    try:\n",
    "        dir_type = 'easy_ham' if row['is_spam'] == 0 else 'spam'\n",
    "        path = os.path.join(DATA_DIR, dir_type, row['filename'])\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            content = f.read().decode('latin-1')\n",
    "        \n",
    "        msg = email.message_from_string(content, policy=policy.default)\n",
    "        \n",
    "        return {\n",
    "            'sender': msg.get('From', None),\n",
    "            'subject': msg.get('Subject', None),\n",
    "            'date': msg.get('Date', None),\n",
    "            'content_type': msg.get('Content-Type', None),\n",
    "            'content': '\\n'.join(\n",
    "                part.get_payload() for part in msg.walk() \n",
    "                if part.get_content_type() == 'text/plain'\n",
    "            )\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['filename']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply feature extraction\n",
    "features = df.apply(extract_email_content, axis=1).apply(pd.Series)\n",
    "df = pd.concat([df, features], axis=1)\n",
    "\n",
    "# Preview extracted features\n",
    "df[['email_id', 'sender', 'subject', 'content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of day extraction\n",
    "def extract_time(hour_str):\n",
    "    try:\n",
    "        parsed = parsedate_tz(hour_str)\n",
    "        return parsed[3] if parsed else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['time_of_day'] = df['date'].apply(extract_time)\n",
    "\n",
    "# Text lengths\n",
    "df['content_length'] = df['content'].str.len().clip(upper=df['content'].str.len().quantile(0.99))\n",
    "df['subject_length'] = df['subject'].str.len().clip(upper=df['subject'].str.len().quantile(0.99))\n",
    "\n",
    "# Combined text feature\n",
    "df['combined_text'] = df[['content', 'subject', 'sender']].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df['time_of_day'] = df['time_of_day'].fillna(df['time_of_day'].median())\n",
    "df = df.dropna(subset=['content'])\n",
    "\n",
    "df[['time_of_day', 'content_length', 'subject_length']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='is_spam', data=df, palette='viridis')\n",
    "plt.title('Spam vs Ham Distribution')\n",
    "plt.xlabel('Email Type (0=Ham, 1=Spam)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of day analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, x='time_of_day', hue='is_spam', fill=True, palette='Set2')\n",
    "plt.title('Email Send Time Distribution')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.xlim(0, 23)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Generation\n",
    "def generate_wordcloud(text, title):\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color='white',\n",
    "                   stop_words=ENGLISH_STOP_WORDS,\n",
    "                   max_words=200).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds\n",
    "generate_wordcloud(' '.join(df[df['is_spam'] == 1]['combined_text']), 'Spam Email Word Cloud')\n",
    "generate_wordcloud(' '.join(df[df['is_spam'] == 0]['combined_text']), 'Ham Email Word Cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[['combined_text', 'time_of_day', 'content_length', 'subject_length']]\n",
    "y = df['is_spam']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(\n",
    "        stop_words=list(ENGLISH_STOP_WORDS),\n",
    "        ngram_range=(1, 2),\n",
    "        max_df=0.9,\n",
    "        min_df=5\n",
    "    ), 'combined_text'),\n",
    "    ('numerical', MinMaxScaler(), ['time_of_day', 'content_length', 'subject_length'])\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        solver='liblinear'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), \n",
    "            annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Ham', 'Spam'],\n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label='Logistic Regression')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
